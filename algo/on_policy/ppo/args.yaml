---
n_experiments: 1 # 1 run the following configuration, otherwise, run test code in parallel see train.py
env:
    name: &env_name BipedalWalker-v2 # LunarLanderContinuous-v2, BipedalWalker-v2
    video_path: video
    log_video: True
    max_episode_steps: &seq_len 1000
    n_envs: 64
    seed: 0
agent:
    algorithm: ppo

    gamma: 0.99
    lam: 0.97
    seq_len: *seq_len
    n_minibatches: 1        # number of minibatches a sequence is divided into 
    # batch size = n_envs * seq_len / n_minibatches
    n_updates: 5            # number of updates per epoch
    n_epochs: 2000
    max_kl: 0.01             # early stop when max_kl is violated. 0 suggests no bound
    mask_data: True
    mask_loss: True
    advantage_type: gae      # norm or gae

    # model path: model_root_dir/model_name/model_name, two model_names ensure each model saved in an independent folder
    # tensorboard path: log_root_dir/model_name
    model_root_dir: saved_models            # root path for savinng models
    log_root_dir: logs  # root path for tensorboard logs
    model_name: baseline

    ac:
        policy_lr: 3e-4
        value_lr: 6e-4
        # network arguments
        norm: layer
        common: True            # if share initial dense and lstm layers, if false common* are treated separately
        common_dense_units: [256]
        common_lstm_units: [256]
        use_lstm: True
        actor_units: [128]
        critic_units: [128]
        # loss arguments
        clip_range: 0.2        # clip range for ppo
        entropy_coef: 0.0
        kl_coef: 0.0
        value_loss_type: clip    # mse or clip
        value_coef: 0.5
        n_value_updates: 6
