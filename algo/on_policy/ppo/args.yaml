---
n_experiments: 1 # 1 run the following configuration, otherwise, run test code in parallel see train.py
env:
    name: &env_name LunarLanderContinuous-v2 # CartPole-v0, LunarLanderContinuous-v2
    max_episode_steps: &seq_len 1000
agent:
    algorithm: ppo
    n_workers: 1
    n_envs: 40

    gamma: 0.99
    lambda: 0.97
    seq_len: *seq_len
    n_minibatches: 10        # #minibatches a sequence is divided into 
    # batch size = seq_len * n_envs * n_workers
    n_updates: 5            # #updates per epoch
    n_epochs: 500

    advantage_type: gae      # norm or gae
    shuffle: False           # shuffle data or not

    # model path: model_root_dir/model_dir/model_name/model_name, two model_names ensure each model saved in an independent folder
    # tensorboard path: log_root_dir/model_dir/model_name
    model_root_dir: data/saved_models            # root path for savinng models
    log_root_dir: data/logs  # root path for tensorboard logs
    model_dir: *env_name
    model_name: adv_norm

    ac:
        learning_rate: 3e-4
        shared_fc_units: 0
        lstm_units: 0
        actor_units: [64, 64]
        clip_range: 0.2        # clip range for ppo
        entropy_coef: 0.0
        critic_units: [64, 64]
        value_coef: 2
        value_loss_type: mse    # mse or clip

    env_stats:
        times: 1
        stats: [score, avg_score, eps_len, avg_eps_len]